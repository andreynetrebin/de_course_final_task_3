# Проект: Анализ данных с использованием PySpark, PostgreSQL и ClickHouse



## Содержание

- [Описание](#описание)
- [Задание](#задание)
- [Стек технологий](#cтек_технологий)
- [Установка](#установка)
- [Использование](#использование)

## Описание



## Задание
Вы работаете в компании, которая управляет крупным интернет-магазином. Ваша задача — создать автоматизированный пайплайн обработки и анализа данных о продажах, используя стек технологий: PostgreSQL, ClickHouse, Apache Airflow и PySpark. Пайплайн должен выполнять генерацию реалистичных данных о продажах, их обработку, очистку, загрузку в базы данных, а также выполнение аналитических операций.

Обратите внимание, что в проекте используются PySpark, Clickhouse, PostgreSQL. PySpark в airflow в docker-compose нету! Его необходимо добавить самостоятельно. Куда вставить в docker-compose - задача самостоятельная. 
Условие задачи следующее - 
Сгенерировать данные о 1 миллионе продаж за последний год.
Каждая запись о продаже должна содержать следующие поля:
- sale_id (уникальный идентификатор продажи).
- customer_id (идентификатор клиента).
- product_id (идентификатор продукта).
- quantity (количество купленных товаров).
- sale_date (дата продажи).
- sale_amount (сумма продажи, рассчитывается как количество товаров * случайная цена товара).
- region (регион клиента, один из: North, South, East, West).

Удалить дубликаты записей о продажах.

Привести данные к нужным форматам для дальнейшей обработки.

Создать таблицу для хранения данных о продажах в PostgreSQL.

Вставить очищенные данные в эту таблицу.

Выполнить агрегацию данных, используя оконные функции и группировки:

- Подсчитать общее количество продаж и сумму продаж для каждого региона и каждого продукта.
- Рассчитать средний чек (average_sale_amount) по регионам и продуктам.
- Сохранить агрегированные данные в отдельную таблицу в PostgreSQL.
- Перенести агрегированные данные из PostgreSQL в ClickHouse. Добавить дату импорта. 

Обращаем Ваше внимание на то, что все операции по созданию таблиц, обработке, миграции данных должны происходить внутри Airflow. Ничего в Dbeaver делать не нужно. Расписание DAG -  12:45 по Москве, каждый вторник.

Где хранить каждый раз файл - задача самостоятельная!

По итогу должен получиться следующий pipeline =
1. DAG запускается.
2. Происходит генерация правдоподобных данные о продажах.
3. Выполняется их очистка и предобработка.
4. Загружаются данные в PostgreSQL, выполняется  необходимые аналитические операции.
5. Переносятся агрегированные данные в ClickHouse.

## Стек технологий

- Apache Airflow
- PySpark
- PostgreSQL
- ClickHouse
- Docker

## Установка


## Использование
