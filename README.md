# Проект: Анализ данных с использованием PySpark, PostgreSQL, ClickHouse и Grafana

Этот проект представляет собой конвейер обработки данных, который включает в себя генерацию, проверку, очистку, загрузку
и анализ данных о продажах. Данные извлекаются из PostgreSQL, обрабатываются с помощью Apache Spark и затем загружаются
в ClickHouse для дальнейшего анализа. Также в проекте создан дашборд в Grafana для визуализации данных.

## Содержание

- [Задание](#задание)
- [Описание](#описание)
- [Стек технологий](#cтек_технологий)
- [Установка](#установка)
- [Использование](использование)
- [Структура проекта](#структура-проекта)
- [Скриншоты](#скриншоты)
- [Лицензия](#лицензия)

## Задание

Вы работаете в компании, которая управляет крупным интернет-магазином. Ваша задача — создать автоматизированный пайплайн
обработки и анализа данных о продажах, используя стек технологий: PostgreSQL, ClickHouse, Apache Airflow и PySpark.
Пайплайн должен выполнять генерацию реалистичных данных о продажах, их обработку, очистку, загрузку в базы данных, а
также выполнение аналитических операций.

Обратите внимание, что в проекте используются PySpark, Clickhouse, PostgreSQL. PySpark в airflow в docker-compose нету!
Его необходимо добавить самостоятельно. Куда вставить в docker-compose - задача самостоятельная.
Условие задачи следующее -
Сгенерировать данные о 1 миллионе продаж за последний год.
Каждая запись о продаже должна содержать следующие поля:

- sale_id (уникальный идентификатор продажи).
- customer_id (идентификатор клиента).
- product_id (идентификатор продукта).
- quantity (количество купленных товаров).
- sale_date (дата продажи).
- sale_amount (сумма продажи, рассчитывается как количество товаров * случайная цена товара).
- region (регион клиента, один из: North, South, East, West).

Удалить дубликаты записей о продажах.

Привести данные к нужным форматам для дальнейшей обработки.

Создать таблицу для хранения данных о продажах в PostgreSQL.

Вставить очищенные данные в эту таблицу.

Выполнить агрегацию данных, используя оконные функции и группировки:

- Подсчитать общее количество продаж и сумму продаж для каждого региона и каждого продукта.
- Рассчитать средний чек (average_sale_amount) по регионам и продуктам.
- Сохранить агрегированные данные в отдельную таблицу в PostgreSQL.
- Перенести агрегированные данные из PostgreSQL в ClickHouse. Добавить дату импорта.

Обращаем Ваше внимание на то, что все операции по созданию таблиц, обработке, миграции данных должны происходить внутри
Airflow. Ничего в Dbeaver делать не нужно. Расписание DAG - 12:45 по Москве, каждый вторник.

Где хранить каждый раз файл - задача самостоятельная!

По итогу должен получиться следующий pipeline =

1. DAG запускается.
2. Происходит генерация правдоподобных данные о продажах.
3. Выполняется их очистка и предобработка.
4. Загружаются данные в PostgreSQL, выполняется необходимые аналитические операции.
5. Переносятся агрегированные данные в ClickHouse.

## Описание

Проект включает в себя следующие этапы обработки данных:

1. **Генерация данных**: Создание тестовых данных о продажах.
2. **Проверка данных**: Проверка корректности и целостности данных.
3. **Очистка данных**: Удаление некорректных или дублирующихся записей.
4. **Загрузка данных в PostgreSQL**: Сохранение очищенных данных в базе данных PostgreSQL.
5. **Анализ данных**: Выполнение аналитических операций над данными.
6. **Перенос данных в ClickHouse**: Перенос данных в ClickHouse для быстрого анализа.

Основной DAG скрипт (`main.py`) управляет выполнением всех этих этапов и запускается по расписанию каждый вторник в 12:
45 по московскому времени.

## Стек технологий

- Python
- Apache Spark
- PostgreSQL
- ClickHouse
- Airflow
- Grafana
- Docker
- Docker Compose

## Установка

1. **Клонируйте репозиторий**:

   ```bash
   git clone https://github.com/andreynetrebin/de_course_final_task_3.git
   cd de_course_final_task_3

2. **Соберите и запустите контейнеры с помощью Docker Compose**:
   Убедитесь, что у вас установлен Docker и Docker Compose. Затем выполните команду:

   ```bash
   docker-compose up --build

3. **Запустите контейнеры**:
   После сборки образов запустите контейнеры:

   ```bash
   docker-compose up

## Использование

1. **Откройте Airflow**:
   После запуска контейнеров, Airflow будет доступен по адресу http://localhost:8080. Войдите в интерфейс Airflow и
   запустите DAG, который будет выполнять все этапы обработки данных.

2. Откройте Grafana и настройте дашборд для визуализации данных. Убедитесь, что Grafana подключена к вашей базе данных
   ClickHouse.

## Структура проекта
```
de_course_final_task_3/
│
├── dags/
│ ├── generate_data.py
│ ├── validate_data.py
│ ├── clean_data.py
│ ├── load_data_to_postgresql.py
│ ├── perform_analytics.py
│ ├── transfer_to_clickhouse.py
│ └── main.py # Основной DAG скрипт
│
├── requirements.txt
├── Dockerfile
├── docker-compose.yml
├── screenshots/
│ ├── airflow_graph.png
│ └── grafana_dashboard.png
└── README.md
```

## Скриншоты

Вот некоторые скриншоты, демонстрирующие работу проекта:

Граф из Airflow: 
![airflow_graph](https://github.com/user-attachments/assets/d2eefe8f-e529-4922-b376-43f1c639025b)

Дашборд из Grafana:
![grafana_dashboard](https://github.com/user-attachments/assets/a286b748-617c-445c-b592-c44d21085a30)


## Лицензия

Этот проект лицензирован под лицензией MIT.
